# 09/14(화) 피어세션

## 1. 학습한 내용

- Attention is all you need, Group Normalization 간단 리뷰
- Attention is not explanation
  ⇒ 기계가 학습하여 결과는 어느 정도 일관성 있게 나오지만, 결과를 도출해내는 과정에서 근거가 되는 word가 다르다면 어떻게 설명이 된다고 볼 수 있는지 다루는 내용.(진선님)

## 2. 학습할 내용

- 수요일까지  
  => transformer관련 논문 및 실습 코드 한 번 더 확인 후 피어세션때 공유.
- 목요일까지:

  - [9강 페이지](https://www.boostcourse.org/boostcampaitech2/lecture/1089695/?isDesc=false)
  - [BERT](https://arxiv.org/abs/1810.04805), [GPT-1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 논문 읽어오기 (발표자들은 발표 준비 해오기❣)

- 금요일까지:
  - [10강 페이지](https://www.boostcourse.org/boostcampaitech2/lecture/1089696/?isDesc=false)
  - [선택 과제2](https://www.boostcourse.org/boostcampaitech2/lecture/1110891?isDesc=false)
  - [선택 과제3](https://www.boostcourse.org/boostcampaitech2/lecture/1110897/?isDesc=false)

## 3. 특이사항

- Positional Encoding 멘토님께 설명 부탁드리기
